Network generate and trained with RNN-LSTM
	- Syntax networks (progress-GB-2)
		Period: 90
		Networks: LSTM(128) - Drop(0.2) - LSTM(64) - Drop(0.2) - LSTM(32) - Drop(0.1) - Flatten - Dense(3)
		loss-func: MSE
		optimizer: SGD (Stochastic Gradient Descent)
		Epochs: 15
	- Syntax networks (progress-GB-3)
		Period: 730
		Networks: LSTM(256) - Drop(0.3) - LSTM(128) - Drop(0.2) - LSTM(64) - Drop(0.2) - LSTM(32) - Drop(0.1) - Flatten - Dense(3)
		loss-func: MSE
		optimizer: ADAM
		Epochs: 15
	- Syntax networks (progress-GB-3-1)
		Period: 30
		Networks: LSTM(256) - Drop(0.3) - LSTM(128) - Drop(0.2) - LSTM(64) - Drop(0.2) - LSTM(32) - Drop(0.1) - Flatten - Dense(3)
		loss-func: MSE
		optimizer: ADAM
		Epochs: 15
	- Syntax networks (progress-GB-3-2)
		Period: 30
		Networks: LSTM(512) - Drop(0.3) - LSTM(256) - Drop(0.25) - LSTM(128) - Drop(0.2) - LSTM(64) - Drop(0.2) - LSTM(32) - Drop(0.1) - Flatten - Dense(3)
		loss-func: MSE
		optimizer: ADAM
		Epochs: 30
		Runtime: 1554.357 Seconds
	- Syntax networks (progress-GB-3-3)
		Period: 30
		Networks: LSTM(512) - Drop(0.3) - LSTM(256) - Drop(0.25) - LSTM(128) - Drop(0.2) - LSTM(64) - Drop(0.2) - LSTM(32) - Drop(0.1) - Flatten - Dense(3)
		loss-func: MSE
		optimizer: ADAM
		Epochs: 15
		Runtime: 733.632 Seconds
	- Syntax networks (progress-GB-3-4)
		Period: 30
		Networks: LSTM(512) - Drop(0.3) - LSTM(256) - Drop(0.25) - LSTM(128) - Drop(0.2) - LSTM(64) - Drop(0.2) - LSTM(32) - Drop(0.1) - Flatten - Dense(3)
		loss-func: MSE
		optimizer: ADAM
		Epochs: 60
		Runtime: 2797.456 seconds
	- Syntax networks (progress-GB-3-5)
		Period: 30
		Networks: LSTM(768) - Drop(0.3) - LSTM(512) - Drop(0.25) - LSTM(256) - Drop(0.25) - LSTM(128) - Drop(0.2) - LSTM(64) - Drop(0.2) - LSTM(32) - Drop(0.1) - Flatten - Dense(3)
		loss-func: MSE
		optimizer: ADAM
		Epochs: 60
		Runtime: 7342.593 seconds

Predict model from x_test data as prediction values and check the error ratio
	AER(Average Error Ratio)
	ME(Maximum Error)
	- Progress-GB-2
		Average Error Ratio:	0.054502	(1.209944 ℃)
		Maximum Error:		0.291207	(6.464818 ℃)
	- *고정 상수값 18.2도 -> 고정 상수값 * 에러량 = 온도 변환
	- Progress-GB-3
		Average Error Ratio:	0.037629	(0.684848 ℃)
		Maximum Error:		0.179944	(3.274981 ℃)
	- Progress-GB-3-1
		Average Error Ratio: 	0.036714	(0.668195 ℃)
		Maximum Error:		0.179271	(3.262732 ℃)
	- Progress-GB-3-2		(3-1에 비해서 Layer를 1개 더 쌓고 Epochs를 15 늘렸음 -> 성능 소폭 향상)
		Average Error Ratio:	0.044456	(0.809099 ℃)
		Maximum Error:		0.207451	(3.775608 ℃)
	- Progress-GB-3-3		(3-2에 비해서 Epochs를 15 낯췄음)
		Average Error Ratio:	0.043274	(0.787587 ℃)	1.67% 성능 하락
		Maximum Error:		0.196007	(3.567327 ℃)	1.93% 성능 하락
	- Progress-GB-3-4		(3-2에 비해서 Epochs를 30 늘렸음)
		Average Error Ratio:	0.041910	(0.762762 ℃)	
		Maximum Error:		0.194708	(3.543686 ℃)
	- Progress-GB-3-5		(3-4에 비해서 Layer를 1개 더 쌓았음)
		Average Error Ratio:	0.041712	(0.759158 ℃)	0.4% 향상
		Maximum Error:		0.175634	(3.196539 ℃)	10.8% 향상